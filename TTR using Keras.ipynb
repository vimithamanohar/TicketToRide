{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from skimage import data\n",
    "import skimage.transform\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "import h5py\n",
    "import math\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implements the identity block\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name base\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    X_shortcut = X\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding=\"VALID\", name=conv_name_base + '2a',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding=\"SAME\", name=conv_name_base + '2b',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding=\"VALID\", name=conv_name_base + '2c',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    # Adding shortcut value to main path, and passing it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "    \"\"\"\n",
    "    Implements convolutional block\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    # First component of main path\n",
    "    X = Conv2D(F1, (1, 1), strides=(s, s), name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding=\"SAME\", name=conv_name_base + '2b',\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding=\"VALID\", name=conv_name_base + \"2c\",\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    # Shortcut path -> Use filters of layer 3 and strides of layer 1\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding=\"VALID\", name=conv_name_base + '1',\n",
    "                        kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Adding shortcut value to main path, and passing it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(64, 64, 3), classes=6):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    nb_filters=64\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(nb_filters, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    # Stage 3\n",
    "    X = convolutional_block(X=X, filters=[128, 128, 512], f=3, s=2, block='a', stage=3)\n",
    "    X = identity_block(X, f=3, filters=[128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, f=3, filters=[128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, f=3, filters=[128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # Stage 4\n",
    "    X = convolutional_block(X=X, filters=[256, 256, 1024], f=3, s=2, block='a', stage=4)\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, f=3, filters=[256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5\n",
    "    X = convolutional_block(X=X, filters=[512, 512, 2048], f=3, s=2, block='a', stage=5)\n",
    "    X = identity_block(X, f=3, filters=[512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, f=3, filters=[512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # average pool\n",
    "    X = AveragePooling2D(pool_size=(2, 2), name='avg_pool')(X)\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read file from one folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "IMG_WIDTH = 759\n",
    "IMG_HEIGHT = 1012\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    #print('file_name: ',path)\n",
    "    images = []\n",
    "    #file_names = sorted(os.listdir(path)) \n",
    "    file_names = sorted(os.listdir(path),key=lambda x: int(os.path.splitext(x)[0]))\n",
    "    for f in file_names:\n",
    "        #print('Image_file_name: ',f)\n",
    "        images.append(data.imread(os.path.join(path, f)))\n",
    "    return images\n",
    "\n",
    "def load_label(filename):    \n",
    "    labels = []\n",
    "    file = open(filename, 'r') \n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        labels.append(line.strip('\\n'))\n",
    "    print('label_file_name written: ',filename)\n",
    "    file.close()\n",
    "    return labels\n",
    "    \n",
    "\n",
    "path = '/home/ubuntu/ttr/TicketToRide'\n",
    "labelfilename='threeTestSegment.txt'\n",
    "train_data_directory=os.path.join(path,'Just3')\n",
    "labelfile=os.path.join(path,labelfilename)\n",
    "    \n",
    "images = load_data(train_data_directory)\n",
    "labels=load_label(labelfile)\n",
    "\n",
    "print(\"Unique Labels: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))\n",
    "\n",
    "\n",
    "# Resize images\n",
    "images32 = [skimage.transform.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                for image in images]\n",
    "#display_images_and_labels(images32, labels)\n",
    "\n",
    "for image in images32[:5]:\n",
    "    print(\"shape: {0}, min: {1}, max: {2}\".format(image.shape, image.min(), image.max()))\n",
    "\n",
    "\n",
    "labels_a = np.array(labels)\n",
    "images_a = np.array(images32)\n",
    "print(\"labels: \", labels_a.shape, \"\\nimages: \", images_a.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read from different folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def load_data_labels(data_directory):\n",
    "    #print('file_name: ',path)\n",
    "    images = []\n",
    "    labels = []\n",
    "    #file_names = sorted(os.listdir(path)) \n",
    "    \n",
    "    directories = [d for d in os.listdir(data_directory) \n",
    "                   if os.path.isdir(os.path.join(data_directory, d))]\n",
    "    for d in directories:\n",
    "        label_value=d.split('class_')[1]\n",
    "        print(label_value)\n",
    "        label_directory = os.path.join(data_directory, d)\n",
    "        \n",
    "        for f in os.listdir(label_directory):\n",
    "            images.append(data.imread(os.path.join(label_directory, f)))\n",
    "            labels.append(label_value)\n",
    "            \n",
    "    return images,labels\n",
    "\n",
    "path = '/home/ubuntu/datasets'\n",
    "train_data_directory=os.path.join(path,'just3')\n",
    "    \n",
    "images, labels = load_data_labels(train_data_directory)\n",
    "print(\"Unique Labels: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))\n",
    "labels_a = np.array(labels)\n",
    "images_a = np.array(images)\n",
    "print(\"labels: \", labels_a.shape, \"\\nimages: \", images_a.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (93, 759, 1012, 3) \n",
      "Test:  (46, 759, 1012, 3) \n",
      "nb_classes:  3\n"
     ]
    }
   ],
   "source": [
    "nb_classes=len(set(labels_a))\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_a, labels_a, test_size=0.33, random_state=42)\n",
    "print(\"Train: \", X_train.shape, \"\\nTest: \", X_test.shape,\"\\nnb_classes: \", nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, id_train=np.unique(y_train,return_inverse=True)\n",
    "Y_train=np_utils.to_categorical(id_train,nb_classes)\n",
    "\n",
    "uniques, id_test=np.unique(y_test,return_inverse=True)\n",
    "Y_test=np_utils.to_categorical(id_test,nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "93/93 [==============================] - 66s 714ms/step - loss: 10.5213 - acc: 0.3011\n",
      "Epoch 2/2\n",
      "93/93 [==============================] - 49s 530ms/step - loss: 10.7454 - acc: 0.3333\n",
      "46/46 [==============================] - 12s 259ms/step\n",
      "Loss = 10.1614081341\n",
      "Test Accuracy = 0.369565219983\n"
     ]
    }
   ],
   "source": [
    "# generate model\n",
    "model = ResNet50(input_shape = (IMG_WIDTH, IMG_HEIGHT, 3), classes = nb_classes)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "model_history = model.fit(X_train, Y_train, epochs = 2, batch_size = 5)\n",
    "\n",
    "# plot accuracy/loss\n",
    "#plot_model_history(model_history)\n",
    "\n",
    "# evaluate model\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
